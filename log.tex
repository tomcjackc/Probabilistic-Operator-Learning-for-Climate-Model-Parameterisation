% LaTeX template for Models of Computation assessed coursework
% Most of the required packages are standard and should be provided by most TeX installations
% The exception is mathpartir, which is provided alongside this document

\documentclass[11pt,a4paper]{article}

\usepackage{fullpage}
\usepackage{rotating}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{stmaryrd}
\usepackage{proof}
% \usepackage{mathpartir}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{url}
\usepackage{multicol}
\usepackage{xr-hyper}

% Common macros

% BNF notation
\newcommand{\Gdef}{\mathrel{\mathop{::}}=}
\newcommand{\Gbar}{\mathbin{\ \big|\ }}
\newcommand{\Coloneqq}{\Gdef}

% Big-step arrow
\newcommand{\bigstep}{\mathrel{\Downarrow}}

% Semantic operators are (often) underlined to avoid ambiguity
\newcommand{\semop}[1]{\mathbin{\underline{#1}}}


% Program syntax is set in teletype using the \cmd macro
\newcommand{\cmd}[1]{\texttt{#1}}

% Macros for program constructs
\newcommand{\ifthen}[3]{
  \cmd{if} \; #1 \; \cmd{then} \; #2 \; \cmd{else} \; #3 }
\newcommand{\while}[2]{
  \cmd{while} \; #1 \; \cmd{do} \; #2 }

% \ang{x} typesets x in angled brackets
% \newcommand{\ang}[1]{\langle #1 \rangle}

% The following two macros are for typesetting rules and derivations
% Usage: \drule{rule name}{premise1 \\ premise2 \\ premise3 ...}{conclusion}
% The premises will often also be derivations using \drule.
% The difference between \drule and and \Drule is that the space for the rule name
% is not measured with \Drule.  This is useful for typesetting left-most subderivations.
\newcommand{\drule}[3]{\inferrule*[left={#1}]{#2}{#3}}
\newcommand{\Drule}[3]{\inferrule*[Left={#1}]{#2}{#3}}

% For defininitions
\newcommand{\eqdef}{\triangleq}


% WhileDM
\newcommand{\whiledm}{\textsc{WhileDM}}
% Names of types
\newcommand{\tname}[1]{\mathit{#1}}
\newcommand{\exprs}{\tname{Expr}}
\newcommand{\bools}{\tname{Bool}}
\newcommand{\comms}{\tname{Comm}}
\newcommand{\vars}{\tname{Var}}
\newcommand{\nums}{\tname{Num}}
\newcommand{\addrs}{\tname{Addr}}
\newcommand{\vals}{\tname{Val}}
\newcommand{\stor}{\tname{Store}}
\newcommand{\heap}{\tname{Heap}}
\newcommand{\bv}{\tname{BVal}}
% \newcommand{\ad}[1]{\ulcorner {#1} \urcorner}
\newcommand{\newp}{\texttt{newpair}}
\newcommand{\fst}[1]{{#1}.\texttt{fst}}
\newcommand{\snd}[1]{{#1}.\texttt{snd}}
% \newcommand{\stof}[2]{\texttt{fst} [ {#1} ] \leftarrow {#2}}
% \newcommand{\stos}[2]{\texttt{snd} [ {#1} ] \leftarrow {#2}}
% \newcommand{\dom}[1]{\mathrm{dom}(#1)}
\newcommand{\bse}{\bigstep_e}
\newcommand{\bsc}{\bigstep_c}
\newcommand{\bsb}{\bigstep_b}
\newcommand{\types}{\tname{Type}}
\newcommand{\typ}{\tau} % Type variable
\newcommand{\tc}{\Gamma} % Type context
\newcommand{\tnat}{\mathsf{nat}}
\newcommand{\tpair}[2]{(#1,#2)}
% \newcommand{\hptyp}[3]{#1 \Vdash #2 : #3}
% \newcommand{\tcompat}[3]{#1 ; #2 ; #3 \vdash \textsf{\textup{well-typed}}}
% \newcommand{\etyp}[3]{#1 \vdash #2 : #3}


\begin{document}
\title{MRes Log}
\author{T. Cowperthwaite}

\maketitle

\section{09/05/24}

\textbf{working in charing cross}

reproduced result sent to Henry on the 7th, although with less strict constraints on kernel and likelihood params. 
result was slightly worse, with test rel L2 = 24\%, with more PCA components being overfit.
unsure of the reason for this, but the values barely move from their initialisation, so maybe an issue with the optimiser?
weird structure in the std is less visible now.

ran the experiment again (n = m = 45, ARD false, multi true, stand true) with the same param constraints (1e-3\textless ls\textless10, 1e-3\textless var\textless 10, 1e-3\textless obs noise), but with initialisations at 2.0 rather than 1.0, on the off-chance that this happens to be a local minimum in some dimensions (maybe due to standardisation?).
there was no overfitting, all components were fit fairly well. 
MLL $\sim$-500.
Test rel L2 = 10\%, visually very good.

Investigating the spatial structure of the PCA components to attempt to explain the structure observed in the std.
the diagonal bands observed in std match the pattern of the PCA14, which was the component of the PCA that was overfit in the first experiment.

Ran experiment with initial values of var and ls = 2.0, obs noise = 1.0, ARD = false, multi = true, stand = true, n = 45, m = 5, although with lengthscale constraint of 1e-3\textless ls\textless20.
MLL values $\sim$-1500, looking promising that this could improve results, obs noise values sticking to lower limit as previously.

Ran same experiment again, this time with upper limit on lengthscale increased to 30.
MLL values $\sim$-1900 now, with obs noise values sticking to lower limit as previously.

Ran same experiment again, this time with upper limit on lengthscale removed.
MLL values $\sim$-2200 now, with obs noise values sticking to lower limit as previously.
Lengthscales selected for the 5 components tested $\sim$60.

Commit 72afcc2.

Looking more at the PCA acting on x and y separately.
for most components, the PCA is generating very similar PCs, but for some components, the PCs are very different, seemingly in antiphase in some cases (including PC14 which was problematic earlier).
This motivates use of a single PCA for both x and y? 
Perhaps using a single PCA for both x and y would allow the model to learn the spatial structure of the data better, as it would be able to learn the spatial structure of the data in both spaces simultaneously, rather than independently, and might make the latent spaces more compatible.
A simple data concatenation could work.

Commit 4431bf9.

Ran full experiment again, with lengthscale constraint of 1e-3\textless ls, and initial values of var and ls = 2.0, obs noise = 1.0, ARD = false, multi = true, stand = true, n = 45, m = 45.
there was no overfitting, all components were fit fairly well. 
MLL ranging from $\sim$-2300 \- $\sim$0.
Test rel L2 = 7.6\%, visually very good.
Lengthscales selected ranged from $\sim$60 for the first few components, to $\sim$20 for the last few.

Commit 96921a5.

\section{10/05/24}

\subsection{Project work}

Motivated by the individual x and y PCA results being similar in most cases, but very different in others (sometimes in antiphase), I implemented the combined PCA by concatenating the x and y training data into a single vector of length 2*4096.
I then fit a 'combined' PCA on the double-length vector, generating a PCA with components made up of directions in 2*4096-dimensional space.
When i interrogated the patterns selected by the PCA in this case, by manually splitting the PCA components, the components were extremely similar in x and y.
The correlations were all now positive and very close to 1, with a gradual decrease with increasing component number, as you would expect as the "lengthscale" of the components gets smaller in physical space.
In the implementation, I manually create self.x pca and self.y pca by extracting the relevant components from self.combined pca and bypassing the fit method.
The x and y latent spaces are now restricted to being the same dimensionality which is no problem.
Running with n = m = 5 yields no errors, will do a full experiment later.

Ran experiment as above, with n = m = 45.
With the same dimensionality as the previous experiment, it is expected that the results will be slightly worse, as the PCA is slightly less optimal on the y data alone, therefore more components will be required in order to capture the same amount of variance.
Test rel L2 = 7.9\%, visually very good.
Lengthscales selected ranged from $\sim$60 for the first few components, to $\sim$20 for the last few.

Added extra plotting to visualise the uncertainty in predicting each PC. 
In the case above, the uncertainty in each PC was very small, until less significant PCs, where the confidence interval became much larger, and contained the true value in almost all cases.

\subsection{Ethics Seminar 1}
\textbf{A whistlestop tour of the ethics of research. Talk given by SJ Beard from CSER.}

Is there a tension between the high standard of research conduct that we aspire to, compared to the level of misconduct required before sanctions are imposed?
I think there is no more tension here than there is in general life, we already have an idea of crime vs deviance. 
Imperfect research practices are not punished, there is a strict standard at which punishments are applied.

\textbf{Consequentialism:}
The idea that the conseqences of your actions are the most important thing.
When you extend this arguement, you find that fundamentally, the consequence that most people want is happiness.
Good use of this theory relies on the idea that each person's happiness is equally important.

\textbf{Deontology:}
The idea that we should all be governed by universal laws of behaviour in a given circumstance.
An example of this is the idea that we should all be governed by a universal law of whether we should lie in a given circumstance, this falls apart as the presence of this law renders the lie useless, and so it is concluded that lying is always wrong.
This is a stricter theory.

\textbf{Contractualism:}
This speaks to the acknowledgement that we are all in a social contract, and that we should all act in a way that is consistent with the rules of this contract.
This is more flexible, and I think that this is the way in which I tend to make my say-to-day decisions.
I like the idea of having a firm belief in what my obligations are, and then having a more free will beyond those obligations.

\textbf{Discussion with Tom and Jakob:}
Jakob not around in meeting, but Tom and I discussed the seminar.
Discussed the ethics surrounding Tom's work using the camera traps, and the potential for the data to be used in ways that were not intended (images of people etc etc).
My work doesn't seem to run into similar problems, but the data used for climate models could have implicit biases (global south vs north etc), so I should be mindful of this when contextualising the work.

\section{13/05/24}

Organise the notebook.

Looking at the combined PCA again, determine the number of PCs required to capture 95\% of variance in x data, and 99\% of the variance in the y data.
Here, we see that the PCA is clearly favouring the y data, as it requires fewer PCs to get a greater fraction of variance.
This is because the x and y data are of different scales, y data tends to be $\sim$7 times larger than x data.

To combat this, I will standardise the x and y data before running the combined PCA, which should remove the bias. 
This could be a best practice to include in the separate PCA steps too.

This is more complicated than I thought, the code is always normalising the explained variance ratio  to sum to 1, even when using very few PCs, therefore I need to identify what the real normalisation is.
I need to calculate the 'total variance' in x train and y train and I'm not sure if this is the standard definition.

After standardising and isolating the 'total variance' in the x and y-specific PCAs, the cumulative variance explained curve is very wacky.
The curve seems normal, with a plateau until around PC 100, then lots of later PCs seem to account for a surprisingly large amount of variance.
I guess strange behaviour is to be expected, as pulling the PCA apart is quite an unnatural operation.
This finding implies that to capture most ($\sim$99\%) of the variance, we would need over 100 PCs on both x and y.

An alternative to consider might be to simply train a PCA on the y data and use it on both the inputs and outputs.
This would mean training less GPs (which is the bottleneck), although they might have to be higher-dimensional in order to capture lots of variance in x.
This is a tradeoff that would have to be made.
The opposite idea would also be valid, training lots of lower-dimensional GPs.
This choice could possibly be made according to the problem at hand?

commit 0dbdad1.

\section{14/05/24}

\subsection{Meeting with Henry}

deep kernel learning

alternative approach: concatenate as new data rather than new features.

apply this new approach to darcy problem - the input and output data look very different.

look at empirical measures of correlation

\subsection{Project work}

Applying the model to the darcy problem, initially without the combined PCA.
Relative L2 = 4.1\%, visually poor, missing sharp features.

\section{15/05/24}

Wrote master script to allow for the mass production of experiments.
Including writing the dataloaders for each experiment individually.

Coded up the combined PCA idea that involves concatenating along the nsamples axis, rather than the nfeatures axis.
This is an alternative way of combining the PCAs, but it's not clear how it would work on problems where the input and output data are very different in shape.

Applied the current model to the helmholtz problem.

For the helmholtz problem, results using this framework are very different depending on whether the data is standardised before the PCA or not, as expected.
This is because the x and y data are of very different scales, and the PCA is biased towards the x data as it exists over larger ranges of values.

\section{16/05/24}

completed a derivation of the combined PCA with an expanded feature space.
it turns out that the framework I coded is equivalent to training a PCA by maximising the sum of the variance of the input data, the variance of the output data, plus two times the covariance between the two.
it will be interesting to try to adapt this theory to allow us to tune the relative importance of the variance vs covariance components of this objective function, while it still has an analytic solution.
Potentially looking at fixing one weight vector and then optimising the other, and then alternating.
Guaranteeing convergance would be interesting.
Would be good to discuss this more with Henry tomorrow.

\section{17/05/24}

\subsection{Project work}

Henry had a similar idea about the joint optimisation of w1 and w2.

\textbf{Reading up about the Rayleigh quotient, which is the quantity that is maximised in the PCA problem.}

\subsection{Ethics seminar 2}

A discussion on the ways in which value judgements can come into research at every stage.
Examples include the choice of research question, the choice of methodology, the choice of data, the choice of analysis, the choice of interpretation, the choice of dissemination.
Journals often have inplicit biases in terms of what topics are considered impactful, and what methodologies are considered valid.

An interesting conclusion drawn was that it is impossible to be completely objective when conducting reasearch, as, at the very least, you are requierd to choose eg. a significance level for hypothesis tests that you conduct.
The best feasible outcome is to ensure transparency and consistency in the choices that you make, and to be open to criticism.

\section{22/05/24}

Had AI4ER showcase yesterday which took up a lot of time in preparation.

\textbf{silly ideas:}
\begin{itemize}
  \item toy equation discovery problem, maybe re-discover $F = ma$ using video analysis and then a simple genetic algorithm (/other comparable approaches??)
  \item use a neural net architecture as a calculator. easy to generate tonnes of training data. can just do pairwise operations, therefore 3 input nodes, one for each number and then one for the operation. would be an interesting test of overfitting etc
\end{itemize}

\subsection{Meeting with Henry}

\textbf{Discussion of the combined PCA.} 
Realisation that the combined PCA in feature space is not as well defined analytically as we thought.
The current implementation does not do the normalisation correctly.
Currently, the constraint implemented is that the SUM of the norms of two weight vectors is 1, whereas the constraint we want to implement is that the norm of each weight vector \textit{individually} is 1, which is a much stronger constraint.
This requires some work to fix, as this is not simply a rescaling due to the definition of the variance that we are maximising, it may also change the "direction" selected.

An interesting alternative analytical approach to formalising this joint PCA is to use lagrange multipliers as used in the PDF Henry sent me.
This seems like a more mechanically robust method to use than the rayleigh quotient.
Look into solving the equation that we ended up with --- maybe using the matrix cookboook for some ideas.

\textbf{Actions:}
\begin{itemize}
  \item Continue with numerical optimisation approach, try to get some solutions for simple problem, before moving onto the toy problems considered so far.
  \item Look at the Lagrange multiplier approach and try to solve that equation to get the solution into closed form (it's not an eigenvalue equation): something like $X_{1}^{T}X_{1}\underline{w}_{1} = \lambda \underline{w}_{1} - X_{1}^{T}X_{2} \underline{w}_{2}$
  \item Quickly generate directly comparable results for the 3 different methods of doing PCA: individual, joint along feature direction, joint along data direction.

\end{itemize}

\textbf{operator learning:}
In order to make this work true operator learning, it should work on data of any resolution within the given domain.
Our approach acheives this through the PCA, so if we fit a smoother to the PCA components we find at a given resolution, we could use these to inform the PCA components at a different resolution.
This would achieve proper operator learning, as the PCA would then be capable of generalising to different resolutions, as a function would.
It would be cool to run some experiments in this direction to see how this works out (darcy problem would be a good testbed for this).
Read through the mesh-invariance section of the paper and try to figure out what they're claiming and how.
We may need to extend these arguements when formalising the joint PCA idea.

realisation: covariance can be negative, we should be looking maximisting the absolute value of the covariance.

\subsection{project work}

\textbf{getting the numerical joint PCA working:}
following the discussion with Henry, it became clear that the normalisation was very important when finding the optimal direction for a given component, due to the definition of variance that we are using (the optimiser might be incentivised to simply make the weight vectors arbitrarily large as this would also act to increase the variance).
the "proper" way to constrain the problem didn't seem to be working, with the weight vector wandering off the unit circle as it got optimised, so i chose to manually normalise the weight vector within the objective function itself.
this was successful at constraining the weight vectors correctly, and the solutions were found quickly for the 2-d toy problem considered, with $\rho \geq 0$.

the effect of changing $\rho$ was very slight, even for very large values, implying to me that there is not much flexibility in such a low-dimensional space for the covariance term to be changed significantly.
the solution for $\rho = 0$ (two independant PCAs) did not seem to be significantly different to the case of the joint PCA, even when the coupling was made large.

I then applied the numerical joint PCA to the toy problems, but the dimensionality proved to be too high for the optimiser to work in a reasonable amount of time. 
I maintain that it would find a solution in principle, but it's impractical to use the numerical scheme for these more realistic operator learning problems, as high-dimensionality is a necessary property of the problem for it to be amenable to an operator-learning approach.

It was decided that more work on an analytic solution should be done, or at least an algorithmic way of finding optimal weight vectors that can be conducted quickly, and we could perhaps demostrate a guarantee of convergance.

\section{23/05/24}

\subsection{project work}

\textbf{figuring out the problem statement for the climate model problem:}
Had another look over Zanna 2020.
My current understanding of the problem they solved is:
\begin{itemize}
  \item They ran a climate/fluids model under some intial conditions and at a high resolution.
  \item They then smoothed the velocity/momentum and temperature fields using a gaussian filter to remove all of the small-scale information.
  \item Then they coarse-grained these smoothed fields to a low-resolution to obtain $\overline{(\mathbf{u}\cdot \nabla)\mathbf{u}}$
  \item Then they run a low-resolution model from the same intial conditions/with the same parameters to obtain $(\bar{\mathbf{u}}\cdot \bar{\nabla})\bar{\mathbf{u}}$
  \item The operator/variable we wish to learn is then $\mathbf{S_u} = (\bar{\mathbf{u}}\cdot \bar{\nabla})\bar{\mathbf{u}} - \overline{(\mathbf{u}\cdot \nabla)\mathbf{u}}$. Which is the parameterisation.
  \item The input to the model must be the output from the low-resolution climate/fluids model, and the perfect output should be the "ideal" low-res model obtained by smoothing and coarse-graining the high-res model as above.
\end{itemize}

In some of this literature, they use $\hat{S}$ to denote the predicted $S$, whereas I automatically read it as an operator when it has a hat.
This is not what they mean, even though this is operator learning.

\section{03/06/24}

let the log slip for a week or so.

Reading a paper (Bhattacharya et al 2021) that seems to detail the same model as what we are using, only with a neural network doing the regression in the latent space, therefore not getting any uncertainty quantification.
They explore the idea of using kernel PCA for dimensionality-reduction --- i'm not sure how this would transfer to the equation-discovery setting (would it still be a closed form solution?).

The Bhattacharya group at caltech does some cool stuff in this space, as well as the Andrew Stuart and Owhadi groups (also caltech). 

\section{project work}
I ran the model on fluids data I generated myself last week.
The model performed very poorly, with huge errors, and visually poor determination of the magnitude of values. 
The size/wavenumber of features seemed to be promising however, so the error could be in the normalisation of the data, even though this does not seem to be a problem in any other models.

Upon inspection of the generated data, some seems to be $\sim$blank for some reason, I need to investigate this, as this could be causing the problem in training.

Is the PV field-to-PV field experiment the best?
I'm not sure if the problem statement as currently laid-out incentivises the model to learn the features that we care about --- we want to learn accurate statistical properties of the "ideal" field for applications to climate models.

Train and test sizes are now 500 and 500 to speed up interating over models.

Rerunning the model on darcy flow problem (r = 30 ie very coarse resolution), n = m = 20, ARD = False, multiinput = True, standardise = True, combine\_pca = False.
Not standardising the data before PCA.
train = 0.03, test = 0.34.
Poor generalisation.

Rerunning with the same params, but with combine\_pca = features.
Not standardising the data before PCA.
train = 0.19, test = 0.26.
Less overfitting, better result at this resolution.
Should try with higher dimensionality now.

Increasing the dimensionality to n = m = 40 has no effect on the results, both train and test were the same.
May increase 'resolution' of the inputs to see if the PCA was struggling to pick up good features.

Run with r = 15, n = m = 40, combine\_pca = features.
No difference.

Run to recreate my previous best result, n = m = 50, combine\_PCA = None, with r = 15.
Train = 0.016, test = 0.345.
Something has gone wrong since last using the Darcy data.

Fixed the Darcy dataloader, there was a typo in the import of the y test data which caused the model accuracy to be misrepresented.

\section{04/06/24}

\subsection{meeting with Henry}

Went through the problems I was having with the real fluids data.

The main outcome was that we found a repo (https://github.com/Pperezhogin/pyqg_generative/blob/master/pyqg_generative/) that contains relevant data for this problem, nicely split into training and testing.
I'm going to use this today to test out the model, it looks reasonably simple.

I also got access to JASMIN today so I can use that for longer runs if required.

\subsection{project work}

Managed to download the data from the globus repository, now I want to run my model on it and see how it performs.

I've got the data into the correct formatting using the code from the paper/repo.
It comes normalised so I hope that works okay with the model, other efforts to standardise have reduced performance so far.

fitted the model with n = m = 10 as a test, with ARD = False, multiinput = True, standardise = True, combine_pca = None and n_train = 10, n_test = 7.
Mean relative L2 was huge ($\sim$300), but the median was around 1.
On some test points (presumably the very early instances of the simulation where ground-truth target values are very small), the error was on the order of thousands, these were skewing the mean L2 score.
I could try again on data that does not originate from the start of simulations in order to avoid this problem.
This would involve slicing the data in the dataloader and wouldn't be too much work.
Alternatively, I could just increase the dimensionality and see how this affects the results, it might be significant.

In general (and for all the toy problems as well), it would be good to have plots that indicate how the test performance varies with a. the number of training examples, and b. the dimensionality of the latent space.

Now running the model again with n = m = 45, as the attempt with n = m = 20 was a slight improvement.
I think I might have to do something cleverer though, such as the time cutoff.

Running with n = m = 45 had very little effect, with the relative L2 just below 1 and the R2 above 0 on train, but the opposite being true on test.
In other words it doesn't seem to be learning very much at all.

\section{06/06/24}
 Lost all of yesterday due to dentist appt and transport issues.

 \subsection{Project work}

\section{13/06/24}

The Lu group at Yale do cool AI4Science stuff --- Lu is the author on the DeepONet paper.




\end{document}
% LaTeX template for Models of Computation assessed coursework
% Most of the required packages are standard and should be provided by most TeX installations
% The exception is mathpartir, which is provided alongside this document

\documentclass[11pt,a4paper]{article}

\usepackage{fullpage}
\usepackage{rotating}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{stmaryrd}
\usepackage{proof}
% \usepackage{mathpartir}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{url}
\usepackage{multicol}
\usepackage{xr-hyper}

% Common macros

% BNF notation
\newcommand{\Gdef}{\mathrel{\mathop{::}}=}
\newcommand{\Gbar}{\mathbin{\ \big|\ }}
\newcommand{\Coloneqq}{\Gdef}

% Big-step arrow
\newcommand{\bigstep}{\mathrel{\Downarrow}}

% Semantic operators are (often) underlined to avoid ambiguity
\newcommand{\semop}[1]{\mathbin{\underline{#1}}}


% Program syntax is set in teletype using the \cmd macro
\newcommand{\cmd}[1]{\texttt{#1}}

% Macros for program constructs
\newcommand{\ifthen}[3]{
  \cmd{if} \; #1 \; \cmd{then} \; #2 \; \cmd{else} \; #3 }
\newcommand{\while}[2]{
  \cmd{while} \; #1 \; \cmd{do} \; #2 }

% \ang{x} typesets x in angled brackets
% \newcommand{\ang}[1]{\langle #1 \rangle}

% The following two macros are for typesetting rules and derivations
% Usage: \drule{rule name}{premise1 \\ premise2 \\ premise3 ...}{conclusion}
% The premises will often also be derivations using \drule.
% The difference between \drule and and \Drule is that the space for the rule name
% is not measured with \Drule.  This is useful for typesetting left-most subderivations.
\newcommand{\drule}[3]{\inferrule*[left={#1}]{#2}{#3}}
\newcommand{\Drule}[3]{\inferrule*[Left={#1}]{#2}{#3}}

% For defininitions
\newcommand{\eqdef}{\triangleq}


% WhileDM
\newcommand{\whiledm}{\textsc{WhileDM}}
% Names of types
\newcommand{\tname}[1]{\mathit{#1}}
\newcommand{\exprs}{\tname{Expr}}
\newcommand{\bools}{\tname{Bool}}
\newcommand{\comms}{\tname{Comm}}
\newcommand{\vars}{\tname{Var}}
\newcommand{\nums}{\tname{Num}}
\newcommand{\addrs}{\tname{Addr}}
\newcommand{\vals}{\tname{Val}}
\newcommand{\stor}{\tname{Store}}
\newcommand{\heap}{\tname{Heap}}
\newcommand{\bv}{\tname{BVal}}
% \newcommand{\ad}[1]{\ulcorner {#1} \urcorner}
\newcommand{\newp}{\texttt{newpair}}
\newcommand{\fst}[1]{{#1}.\texttt{fst}}
\newcommand{\snd}[1]{{#1}.\texttt{snd}}
% \newcommand{\stof}[2]{\texttt{fst} [ {#1} ] \leftarrow {#2}}
% \newcommand{\stos}[2]{\texttt{snd} [ {#1} ] \leftarrow {#2}}
% \newcommand{\dom}[1]{\mathrm{dom}(#1)}
\newcommand{\bse}{\bigstep_e}
\newcommand{\bsc}{\bigstep_c}
\newcommand{\bsb}{\bigstep_b}
\newcommand{\types}{\tname{Type}}
\newcommand{\typ}{\tau} % Type variable
\newcommand{\tc}{\Gamma} % Type context
\newcommand{\tnat}{\mathsf{nat}}
\newcommand{\tpair}[2]{(#1,#2)}
% \newcommand{\hptyp}[3]{#1 \Vdash #2 : #3}
% \newcommand{\tcompat}[3]{#1 ; #2 ; #3 \vdash \textsf{\textup{well-typed}}}
% \newcommand{\etyp}[3]{#1 \vdash #2 : #3}


\begin{document}
\title{MRes Log}
\author{T. Cowperthwaite}

\maketitle

\section{09/05/24}

\textbf{working in charing cross}

reproduced result sent to Henry on the 7th, although with less strict constraints on kernel and likelihood params. 
result was slightly worse, with test rel L2 = 24\%, with more PCA components being overfit.
unsure of the reason for this, but the values barely move from their initialisation, so maybe an issue with the optimiser?
weird structure in the std is less visible now.

ran the experiment again (n = m = 45, ARD false, multi true, stand true) with the same param constraints (1e-3\textless ls\textless10, 1e-3\textless var\textless 10, 1e-3\textless obs noise), but with initialisations at 2.0 rather than 1.0, on the off-chance that this happens to be a local minimum in some dimensions (maybe due to standardisation?).
there was no overfitting, all components were fit fairly well. 
MLL $\sim$-500.
Test rel L2 = 10\%, visually very good.

Investigating the spatial structure of the PCA components to attempt to explain the structure observed in the std.
the diagonal bands observed in std match the pattern of the PCA14, which was the component of the PCA that was overfit in the first experiment.

Ran experiment with initial values of var and ls = 2.0, obs noise = 1.0, ARD = false, multi = true, stand = true, n = 45, m = 5, although with lengthscale constraint of 1e-3\textless ls\textless20.
MLL values $\sim$-1500, looking promising that this could improve results, obs noise values sticking to lower limit as previously.

Ran same experiment again, this time with upper limit on lengthscale increased to 30.
MLL values $\sim$-1900 now, with obs noise values sticking to lower limit as previously.

Ran same experiment again, this time with upper limit on lengthscale removed.
MLL values $\sim$-2200 now, with obs noise values sticking to lower limit as previously.
Lengthscales selected for the 5 components tested $\sim$60.

Commit 72afcc2.

Looking more at the PCA acting on x and y separately.
for most components, the PCA is generating very similar PCs, but for some components, the PCs are very different, seemingly in antiphase in some cases (including PC14 which was problematic earlier).
This motivates use of a single PCA for both x and y? 
Perhaps using a single PCA for both x and y would allow the model to learn the spatial structure of the data better, as it would be able to learn the spatial structure of the data in both spaces simultaneously, rather than independently, and might make the latent spaces more compatible.
A simple data concatenation could work.

Commit 4431bf9.

Ran full experiment again, with lengthscale constraint of 1e-3\textless ls, and initial values of var and ls = 2.0, obs noise = 1.0, ARD = false, multi = true, stand = true, n = 45, m = 45.
there was no overfitting, all components were fit fairly well. 
MLL ranging from $\sim$-2300 \- $\sim$0.
Test rel L2 = 7.6\%, visually very good.
Lengthscales selected ranged from $\sim$60 for the first few components, to $\sim$20 for the last few.

Commit 96921a5.

\section{10/05/24}

\subsection{Project work}

Motivated by the individual x and y PCA results being similar in most cases, but very different in others (sometimes in antiphase), I implemented the combined PCA by concatenating the x and y training data into a single vector of length 2*4096.
I then fit a 'combined' PCA on the double-length vector, generating a PCA with components made up of directions in 2*4096-dimensional space.
When i interrogated the patterns selected by the PCA in this case, by manually splitting the PCA components, the components were extremely similar in x and y.
The correlations were all now positive and very close to 1, with a gradual decrease with increasing component number, as you would expect as the "lengthscale" of the components gets smaller in physical space.
In the implementation, I manually create self.x_pca and self.y_pca by extracting the relevant components from self.combined_pca and bypassing the fit method.
The x and y latent spaces are now restricted to being the same dimensionality which is no problem.
Running with n = m = 5 yields no errors, will do a full experiment later.

Ran experiment as above, with n = m = 45.
With the same dimensionality as the previous experiment, it is expected that the results will be slightly worse, as the PCA is slightly less optimal on the y data alone, therefore more components will be required in order to capture the same amount of variance.
Test rel L2 = 7.9\%, visually very good.
Lengthscales selected ranged from $\sim$60 for the first few components, to $\sim$20 for the last few.

Added extra plotting to visualise the uncertainty in predicting each PC. 
In the case above, the uncertainty in each PC was very small, until less significant PCs, where the confidence interval became much larger, and contained the true value in almost all cases.

\subsection{Ethics Seminar 1}
\textbf{A whistlestop tour of the ethics of research. Talk given by SJ Beard from CSER.}

Is there a tension between the high standard of research conduct that we aspire to, compared to the level of misconduct required before sanctions are imposed?
I think there is no more tension here than there is in general life, we already have an idea of crime vs deviance. 
Imperfect research practices are not punished, there is a strict standard at which punishments are applied.

\textbf{Consequentialism:}
The idea that the conseqences of your actions are the most important thing.
When you extend this arguement, you find that fundamentally, the consequence that most people want is happiness.
Good use of this theory relies on the idea that each person's happiness is equally important.

\textbf{Deontology:}
The idea that we should all be governed by universal laws of behaviour in a given circumstance.
An example of this is the idea that we should all be governed by a universal law of whether we should lie in a given circumstance, this falls apart as the presence of this law renders the lie useless, and so it is concluded that lying is always wrong.
This is a stricter theory.

\textbf{Contractualism:}
This speaks to the acknowledgement that we are all in a social contract, and that we should all act in a way that is consistent with the rules of this contract.
This is more flexible, and I think that this is the way in which I tend to make my say-to-day decisions.
I like the idea of having a firm belief in what my obligations are, and then having a more free will beyond those obligations.

\textbf{Discussion with Tom and Jakob:}
Jakob not around in meeting, but Tom and I discussed the seminar.
Discussed the ethics surrounding Tom's work using the camera traps, and the potential for the data to be used in ways that were not intended (images of people etc etc).
My work doesn't seem to run into similar problems, but the data used for climate models could have implicit biases (global south vs north etc), so I should be mindful of this when contextualising the work.

\section{13/05/24}

Organise the notebook.

Looking at the combined PCA again, determine the number of PCs required to capture 95\% of variance in x data, and 99\% of the variance in the y data.
Here, we see that the PCA is clearly favouring the y data, as it requires fewer PCs to get a greater fraction of variance.
This is because the x and y data are of different scales, y data tends to be $\sim$7 times larger than x data.

To combat this, I will standardise the x and y data before running the combined PCA, which should remove the bias. 
This could be a best practice to include in the separate PCA steps too.

This is more complicated than I thought, the code is always normalising the explained_variance_ratio_ to sum to 1, even when using very few PCs, therefore I need to identify what the real normalisation is.
I need to calculate the 'total variance' in x_train and y_train and I'm not sure if this is the standard definition.

After standardising and isolating the 'total variance' in the x and y-specific PCAs, the cumulative variance explained curve is very wacky.
The curve seems normal, with a plateau until around PC 100, then lots of later PCs seem to account for a surprisingly large amount of variance.
I guess strange behaviour is to be expected, as pulling the PCA apart is quite an unnatural operation.
This finding implies that to capture most ($\sim$99\%) of the variance, we would need over 100 PCs on both x and y.

An alternative to consider might be to simply train a PCA on the y data and use it on both the inputs and outputs.
This would mean training less GPs (which is the bottleneck), although they might have to be higher-dimensional in order to capture lots of variance in x.
This is a tradeoff that would have to be made.
The opposite idea would also be valid, training lots of lower-dimensional GPs.
This choice could possibly be made according to the problem at hand?



\end{document}